{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Deep-Learning-12 to yolov11:: 100%|██████████| 22689/22689 [00:01<00:00, 14518.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Deep-Learning-12 in yolov11:: 100%|██████████| 1128/1128 [00:00<00:00, 2943.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"yAGqrP4bbcFc0kgTqBks\")\n",
    "project = rf.workspace(\"aarhus-universitet-r1lgs\").project(\"deep-learning-wnuyb\")\n",
    "version = project.version(12)\n",
    "dataset = version.download(\"yolov11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"./Deep-learning-12/train\") \n",
    "labels = sorted(dataset_path.rglob(\"*labels/*.txt\"))  # all data in 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_file = \"./Deep-learning-12/data.yaml\"  \n",
    "with open(yaml_file, \"r\", encoding=\"utf8\") as y:\n",
    "    classes = yaml.safe_load(y)[\"names\"]\n",
    "cls_idx = sorted(classes.keys())\n",
    "\n",
    "print(cls_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "indx = [label.stem for label in labels]  # uses base filename as ID (no extension)\n",
    "labels_df = pd.DataFrame([], columns=cls_idx, index=indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     0    1    2    3    4   \\\n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...  NaN  NaN  NaN  NaN  NaN   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc      NaN  NaN  NaN  NaN  NaN   \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08      NaN  NaN  NaN  NaN  NaN   \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       NaN  NaN  NaN  NaN  NaN   \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb       NaN  NaN  NaN  NaN  NaN   \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "                                                     5    6    7    8    9   \\\n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...  NaN  NaN  NaN  NaN  NaN   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc      NaN  NaN  NaN  NaN  NaN   \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08      NaN  NaN  NaN  NaN  NaN   \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       NaN  NaN  NaN  NaN  NaN   \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb       NaN  NaN  NaN  NaN  NaN   \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "                                                     10   11   12   13   14  \\\n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...  NaN  NaN  NaN  NaN  NaN   \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...  NaN  NaN  NaN  NaN  NaN   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc      NaN  NaN  NaN  NaN  NaN   \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08      NaN  NaN  NaN  NaN  NaN   \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       NaN  NaN  NaN  NaN  NaN   \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb       NaN  NaN  NaN  NaN  NaN   \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "                                                     15   16   17   18   19  \n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...  NaN  NaN  NaN  NaN  NaN  \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...  NaN  NaN  NaN  NaN  NaN  \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...  NaN  NaN  NaN  NaN  NaN  \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...  NaN  NaN  NaN  NaN  NaN  \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...  NaN  NaN  NaN  NaN  NaN  \n",
      "...                                                 ...  ...  ...  ...  ...  \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc      NaN  NaN  NaN  NaN  NaN  \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08      NaN  NaN  NaN  NaN  NaN  \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       NaN  NaN  NaN  NaN  NaN  \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb       NaN  NaN  NaN  NaN  NaN  \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...  NaN  NaN  NaN  NaN  NaN  \n",
      "\n",
      "[561 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/v36xrkmd52s39s9cz2xzy7t80000gn/T/ipykernel_10640/4074969495.py:15: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  labels_df = labels_df.fillna(0.0)  # replace `nan` values with `0.0`\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for label in labels:\n",
    "    lbl_counter = Counter()\n",
    "\n",
    "    with open(label, \"r\") as lf:\n",
    "        lines = lf.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # classes for YOLO label uses integer at first position of each line\n",
    "        lbl_counter[int(line.split(\" \")[0])] += 1\n",
    "\n",
    "    labels_df.loc[label.stem] = lbl_counter\n",
    "\n",
    "labels_df = labels_df.fillna(0.0)  # replace `nan` values with `0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     0    1    2    3    4   \\\n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...  1.0  0.0  0.0  0.0  0.0   \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...  4.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...  1.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...  1.0  0.0  0.0  0.0  0.0   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc      0.0  0.0  0.0  0.0  0.0   \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08      0.0  0.0  0.0  0.0  0.0   \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       0.0  0.0  0.0  0.0  0.0   \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb       0.0  0.0  0.0  0.0  0.0   \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "                                                     5    6    7    8    9   \\\n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...  0.0  1.0  0.0  0.0  0.0   \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...  0.0  0.0  0.0  0.0  0.0   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc      0.0  0.0  0.0  0.0  0.0   \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08      0.0  0.0  0.0  0.0  0.0   \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       0.0  0.0  0.0  0.0  0.0   \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb       0.0  0.0  0.0  0.0  0.0   \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "                                                     10   11   12   13   14  \\\n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...  0.0  0.0  0.0  0.0  0.0   \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...  0.0  0.0  0.0  0.0  0.0   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc      0.0  0.0  0.0  0.0  0.0   \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08      0.0  0.0  0.0  0.0  0.0   \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       0.0  0.0  0.0  0.0  0.0   \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb       0.0  0.0  0.0  0.0  0.0   \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "                                                      15   16   17   18   19  \n",
      "AEBLER_I_POSE_png.rf.a73452dcbe3095dda703647be2...   0.0  0.0  0.0  0.0  0.0  \n",
      "AEGTE_MAYONNAISE_png.rf.4c4f2cfa18214e9af5aad1e...   0.0  0.0  0.0  0.0  0.0  \n",
      "AEble_Belle_de_Boskoop_oko-_1_png.rf.2e9d1f7f78...   0.0  0.0  0.0  0.0  0.0  \n",
      "AEble_Belle_de_Boskoop_stor_1_png.rf.5a09d80e53...   0.0  0.0  0.0  0.0  0.0  \n",
      "AEble_Royal_Gala_1_png.rf.3c680fb79e6491fc6b4e4...   0.0  0.0  0.0  0.0  0.0  \n",
      "...                                                  ...  ...  ...  ...  ...  \n",
      "images_jpg.rf.aab537fa8ad02a17ae4da3028a9577dc       1.0  0.0  0.0  0.0  0.0  \n",
      "images_jpg.rf.af5f769c9f1cbf9ae739b038a314bc08       0.0  0.0  1.0  0.0  0.0  \n",
      "piske_jpg.rf.10c76dabef39c461cb23354afdd2e99d       12.0  0.0  0.0  0.0  0.0  \n",
      "piske_jpg.rf.cc045bca5130c7e19b85793579cd0dcb        3.0  0.0  0.0  0.0  0.0  \n",
      "smagstest-af-rugbroed_jpg.rf.4315ba178c969b3f3f...   0.0  8.0  0.0  0.0  0.0  \n",
      "\n",
      "[561 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# display non-zero values\n",
    "print(labels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "ksplit = 5\n",
    "kf = KFold(n_splits=ksplit, shuffle=True, random_state=20)  # setting random_state for repeatable results\n",
    "\n",
    "kfolds = list(kf.split(labels_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/v36xrkmd52s39s9cz2xzy7t80000gn/T/ipykernel_10640/1666497045.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  folds_df[f\"split_{idx}\"].loc[labels_df.iloc[train].index] = \"train\"\n",
      "/var/folders/lq/v36xrkmd52s39s9cz2xzy7t80000gn/T/ipykernel_10640/1666497045.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  folds_df[f\"split_{idx}\"].loc[labels_df.iloc[val].index] = \"val\"\n"
     ]
    }
   ],
   "source": [
    "folds = [f\"split_{n}\" for n in range(1, ksplit + 1)]\n",
    "folds_df = pd.DataFrame(index=indx, columns=folds)\n",
    "\n",
    "for idx, (train, val) in enumerate(kfolds, start=1):\n",
    "    folds_df[f\"split_{idx}\"].loc[labels_df.iloc[train].index] = \"train\"\n",
    "    folds_df[f\"split_{idx}\"].loc[labels_df.iloc[val].index] = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_lbl_distrb = pd.DataFrame(index=folds, columns=cls_idx)\n",
    "\n",
    "for n, (train_indices, val_indices) in enumerate(kfolds, start=1):\n",
    "    train_totals = labels_df.iloc[train_indices].sum()\n",
    "    val_totals = labels_df.iloc[val_indices].sum()\n",
    "\n",
    "    # To avoid division by zero, we add a small value (1E-7) to the denominator\n",
    "    ratio = val_totals / (train_totals + 1e-7)\n",
    "    fold_lbl_distrb.loc[f\"split_{n}\"] = ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "supported_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "\n",
    "# Initialize an empty list to store image file paths\n",
    "images = []\n",
    "\n",
    "# Loop through supported extensions and gather image files\n",
    "for ext in supported_extensions:\n",
    "    images.extend(sorted((dataset_path / \"images\").rglob(f\"*{ext}\")))\n",
    "\n",
    "# Create the necessary directories and dataset YAML files (unchanged)\n",
    "save_path = Path(dataset_path / f\"{datetime.date.today().isoformat()}_{ksplit}-Fold_Cross-val\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "ds_yamls = []\n",
    "\n",
    "for split in folds_df.columns:\n",
    "    # Create directories\n",
    "    split_dir = save_path / split\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (split_dir / \"train\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (split_dir / \"train\" / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "    (split_dir / \"val\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (split_dir / \"val\" / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create dataset YAML files\n",
    "    dataset_yaml = split_dir / f\"{split}_dataset.yaml\"\n",
    "    ds_yamls.append(dataset_yaml)\n",
    "\n",
    "    with open(dataset_yaml, \"w\") as ds_y:\n",
    "        yaml.safe_dump(\n",
    "            {\n",
    "                \"path\": split_dir.as_posix(),\n",
    "                \"train\": \"train\",\n",
    "                \"val\": \"val\",\n",
    "                \"names\": classes,\n",
    "            },\n",
    "            ds_y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    for split, k_split in folds_df.loc[image.stem].items():\n",
    "        # Destination directory\n",
    "        img_to_path = save_path / split / k_split / \"images\"\n",
    "        lbl_to_path = save_path / split / k_split / \"labels\"\n",
    "\n",
    "        # Copy image and label files to new directory (SamefileError if file already exists)\n",
    "        shutil.copy(image, img_to_path / image.name)\n",
    "        shutil.copy(label, lbl_to_path / label.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "weights_path = \"./yolo11n.pt\"\n",
    "model = YOLO(weights_path, task=\"detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.39 🚀 Python-3.9.6 torch-2.5.1 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=./yolo11n.pt, data=Deep-learning-12/train/2024-11-29_5-Fold_Cross-val/split_1/split_1_dataset.yaml, epochs=2, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=kfold_demo, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=kfold_demo/train4\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    434572  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "YOLO11n summary: 319 layers, 2,593,740 parameters, 2,593,724 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets/Deep-learning-12/train/2024-11-29_5-Fold_Cross-val/split_1/train/labels... 448 images, 0 backgrounds, 0 corrupt: 100%|██████████| 448/448 [00:00<00:00, 979.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets/Deep-learning-12/train/2024-11-29_5-Fold_Cross-val/split_1/train/labels.cache\n",
      "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 1, len(boxes) = 654. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets/Deep-learning-12/train/2024-11-29_5-Fold_Cross-val/split_1/val/labels... 113 images, 0 backgrounds, 0 corrupt: 100%|██████████| 113/113 [00:00<00:00, 2429.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets/Deep-learning-12/train/2024-11-29_5-Fold_Cross-val/split_1/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to kfold_demo/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mkfold_demo/train4\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2         0G      1.072       4.11      1.513         45        640:  18%|█▊        | 5/28 [00:49<03:47,  9.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m dataset_yaml \u001b[38;5;241m=\u001b[39m ds_yamls[k]\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(weights_path, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# include any train arguments\u001b[39;00m\n\u001b[1;32m     12\u001b[0m results[k] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmetrics  \u001b[38;5;66;03m# save output metrics for further analysis\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/engine/model.py:805\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:388\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    384\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Define your additional arguments here\n",
    "batch = 16\n",
    "project = \"kfold_demo\"\n",
    "epochs = 2\n",
    "\n",
    "for k in range(ksplit):\n",
    "    dataset_yaml = ds_yamls[k]\n",
    "    model = YOLO(weights_path, task=\"detect\")\n",
    "    model.train(data=dataset_yaml, epochs=epochs, batch=batch, project=project)  # include any train arguments\n",
    "    results[k] = model.metrics  # save output metrics for further analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train YOLOv11 with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/Users/NichlasKondrup/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/NichlasKondrup/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=Deep-Learning-12/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'Deep-Learning-12/data.yaml' error ❌ \nDataset 'Deep-Learning-12/data.yaml' images not found ⚠️, missing path '/Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets/Deep-Learning-12/valid/images'\nNote dataset download directory is '/Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets'. You can update this in '/Users/NichlasKondrup/Library/Application Support/Ultralytics/settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:562\u001b[0m, in \u001b[0;36mBaseTrainer.get_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myml\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    561\u001b[0m }:\n\u001b[0;32m--> 562\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/data/utils.py:329\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[0;34m(dataset, autodownload)\u001b[0m\n\u001b[1;32m    328\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote dataset download directory is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can update this in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[1;32m    330\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: \nDataset 'Deep-Learning-12/data.yaml' images not found ⚠️, missing path '/Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets/Deep-Learning-12/valid/images'\nNote dataset download directory is '/Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets'. You can update this in '/Users/NichlasKondrup/Library/Application Support/Ultralytics/settings.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model using the 'coco8.yaml' dataset for 3 epochs\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDeep-Learning-12/data.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the validation set\u001b[39;00m\n\u001b[1;32m     20\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mval()\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/engine/model.py:799\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    797\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[0;32m--> 799\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:133\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/2/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:566\u001b[0m, in \u001b[0;36mBaseTrainer.get_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m error ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset 'Deep-Learning-12/data.yaml' error ❌ \nDataset 'Deep-Learning-12/data.yaml' images not found ⚠️, missing path '/Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets/Deep-Learning-12/valid/images'\nNote dataset download directory is '/Users/NichlasKondrup/Desktop/Deep Learning/2/Deep_learning_project/datasets'. You can update this in '/Users/NichlasKondrup/Library/Application Support/Ultralytics/settings.json'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Create a new YOLO model from scratch\n",
    "model = YOLO(\"yolo11n.yaml\")\n",
    "\n",
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "# Train the model using the 'coco8.yaml' dataset for 3 epochs\n",
    "results = model.train(data=\"Deep-Learning-12/data.yaml\", epochs=10)\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "results = model.val()\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "#results = model(\"https://ultralytics.com/images/bus.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Bruger\\OneDrive\\Dokumenter\\PyJects\\Project\\yolov5\\data\\images\\lettuce_test.jpg: 512x640 1 hakket oksekod, 1 salat, 26.0ms\n",
      "Speed: 0.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(source=\"data/images\",conf=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 7., 17.], device='cuda:0')\n",
      "conf: tensor([0.32103, 0.27564], device='cuda:0')\n",
      "data: tensor([[4.19142e+01, 4.11390e+00, 6.25132e+02, 5.31442e+02, 3.21026e-01, 7.00000e+00],\n",
      "        [2.90360e+01, 3.79087e+00, 6.29693e+02, 5.30580e+02, 2.75636e-01, 1.70000e+01]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (538, 674)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[333.52310, 267.77808, 583.21783, 527.32837],\n",
      "        [329.36441, 267.18546, 600.65680, 526.78912]], device='cuda:0')\n",
      "xywhn: tensor([[0.49484, 0.49773, 0.86531, 0.98016],\n",
      "        [0.48867, 0.49663, 0.89118, 0.97916]], device='cuda:0')\n",
      "xyxy: tensor([[ 41.91417,   4.11390, 625.13202, 531.44226],\n",
      "        [ 29.03601,   3.79087, 629.69281, 530.58002]], device='cuda:0')\n",
      "xyxyn: tensor([[0.06219, 0.00765, 0.92750, 0.98781],\n",
      "        [0.04308, 0.00705, 0.93426, 0.98621]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    print(boxes)\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
